---
description: Install Calico Enterprise on an MKE cluster.
---

# Mirantis Kubernetes Engine (MKE)

import DockerEe from '@site/calico-enterprise_versioned_docs/version-3.14/_includes/content/_docker-ee.mdx';

## Big picture

Install {{prodname}} on a Mirantis Kubernetes Engine (MKE) cluster (formerly Docker Enterprise).

## Before you begin

**MKE requirements**

- <DockerEe /> install with:

  - A minimum of three nodes for non-production deployments
  - CNI flag set to unmanaged, `--unmanaged-cni` so UCP does not install the default {{prodname}} networking plugin

  For help, see [Docker Enterprise](https://docs.docker.com/) and [Docker EE Best Practices and Design Considerations](https://docs.mirantis.com/docker-enterprise/v3.0/dockeree-ref-arch/deploy-manage/best-practices-design.html)

- Install UCP control plane to access the cluster using [Docker Universal Control Plane CLI-Based Access](https://dockerlabs.collabnix.com/advanced/Docker-UCP-overview.html). After installing the control plane, enable the option "Allow all authenticated users, including service accounts, to schedule on all nodes, including UCP managers and DTR nodes."

**{{prodname}} requirements**

- Your MKE cluster meets the [{{prodname}} system requirements](requirements.mdx)

- If using a private registry, familiarize yourself with this guide on [using a private registry](private-registry/index.mdx).

- [Credentials for the Tigera private registry and a license key](calico-enterprise.mdx)

- Install `kubectl` CLI tool. See [Install kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)

## How to

The geeky details of what you get:

<GeekDetails
  prodname='{{prodname}}'
  details='Policy:Calico,IPAM:Calico,CNI:Calico,Overlay:IPIP,Routing:BGP,Datastore:Kubernetes'
/>

- [Install {{prodname}}](#install-calico-enterprise)
- [Install the {{prodname}} license](#install-the-calico-enterprise-license)
- [Secure {{prodname}} with network policy](#secure-calico-enterprise-with-network-policy)

### Install {{prodname}}

1. [Configure a storage class for {{prodname}}](../../operations/logstorage/create-storage.mdx).

1. Configure Tigera operator role bindings for Docker EE.

   ```
   kubectl create clusterrolebinding tigera-operator-cluster-admin -n tigera-operator \
    --clusterrole cluster-admin --serviceaccount tigera-operator:tigera-operator
   ```

1. Install the Tigera operator and custom resource definitions.

   ```
   kubectl create -f {{filesUrl}}/manifests/tigera-operator.yaml
   ```

1. Install the Prometheus operator and related custom resource definitions. The Prometheus operator will be used to deploy Prometheus server and Alertmanager to monitor {{prodname}} metrics.

   :::note

   If you have an existing Prometheus operator in your cluster that you want to use, skip this step. To work with {{prodname}}, your Prometheus operator must be v0.40.0 or higher.

   :::

   ```
   kubectl create -f {{filesUrl}}/manifests/tigera-prometheus-operator.yaml
   ```

1. Install your pull secret.

   If pulling images directly from `quay.io/tigera`, you will likely want to use the credentials provided to you by your Tigera support representative. If using a private registry, use your private registry credentials instead.

   ```
   kubectl create secret generic tigera-pull-secret \
       --type=kubernetes.io/dockerconfigjson -n tigera-operator \
       --from-file=.dockerconfigjson=<path/to/pull/secret>
   ```

   For the Prometheus operator, create the pull secret in the `tigera-prometheus` namespace and then patch the deployment.

   ```
   kubectl create secret generic tigera-pull-secret \
       --type=kubernetes.io/dockerconfigjson -n tigera-prometheus \
       --from-file=.dockerconfigjson=<path/to/pull/secret>
   kubectl patch deployment -n tigera-prometheus calico-prometheus-operator \
       -p '{"spec":{"template":{"spec":{"imagePullSecrets":[{"name": "tigera-pull-secret"}]}}}}'
   ```

1. Install any extra [{{prodname}} resources](../../reference/resources/index.mdx) needed at cluster start using [calicoctl](../../reference/clis/calicoctl/overview.mdx).

1. Install the Tigera custom resources. For more information on configuration options available in this manifest, see [the installation reference](../../reference/installation/api.mdx).

   ```
   kubectl create -f {{filesUrl}}/manifests/custom-resources.yaml
   ```

   You can now monitor progress with the following command:

   ```
   watch kubectl get tigerastatus
   ```

   Wait until the `apiserver` shows a status of `Available`, then proceed to the next section.

### Install the {{prodname}} license

In order to use {{prodname}}, you must install the license provided to you by Tigera.

```
kubectl create -f </path/to/license.yaml>
```

You can now monitor progress with the following command:

```
watch kubectl get tigerastatus
```

When all components show a status of `Available`, proceed to the next section.

### Secure {{prodname}} with network policy

To secure {{prodname}} component communications, install the following set of network policies.

```
kubectl create -f {{filesUrl}}/manifests/tigera-policies.yaml
```

## Next steps

**Recommended**

- [Configure access to {{prodname}} Manager UI](../../operations/cnx/access-the-manager.mdx)
- [Authentication quickstart](../../operations/cnx/authentication-quickstart.mdx)
- [Configure an external identity provider](../../operations/cnx/configure-identity-provider.mdx)

**Recommended - Networking**

- The default networking uses IP-in-IP with BGP routing. For all networking options, see [Determine best networking option](../../networking/determine-best-networking.mdx).

**Recommended - Security**

- [Get started with {{prodname}} tiered network policy](../../network-policy/policy-tiers/tiered-policy.mdx)
