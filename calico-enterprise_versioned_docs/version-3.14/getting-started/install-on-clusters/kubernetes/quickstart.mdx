---
description: Install Calico Enterprise on a single-host Kubernetes cluster for testing or development.
---

# Quickstart for Calico Enterprise on Kubernetes

## Big picture

This quickstart gets you a single-host Kubernetes cluster with {{prodname}} in approximately 15 minutes.

## Value

Use this quickstart to quickly and easily try {{prodname}} features. To deploy a cluster suitable for production, refer to [{{prodname}} on Kubernetes](../kubernetes/index.mdx).

## Concepts

### Operator based installation

This quickstart guide uses the Tigera operator to install {{prodname}}. The operator provides lifecycle management for {{prodname}} exposed via the Kubernetes API defined as a custom resource definition.

## Before you begin

**Required**

A Linux host that meets the following requirements.

- x86-64
- 2CPU
- 12GB RAM
- 50GB free disk space
- Ubuntu Server 18.04
- Internet access
- [Sufficient virtual memory](https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html)

## How to

The geeky details of what you get:

<GeekDetails
  prodname='{{prodname}}'
  details='Policy:Calico,IPAM:Calico,CNI:Calico,Overlay:IPIP,Routing:BGP,Datastore:kubernetes'
/>

- [Install Kubernetes](#install-kubernetes)
- [Install {{prodname}}](#install-calico-enterprise)
- [Install the {{prodname}} license](#install-the-calico-enterprise-license)
- [Log in to {{prodname}} Manager](#log-in-to-calico-enterprise-manager)
- [Secure {{prodname}} with network policy](#secure-calico-enterprise-with-network-policy)

### Install Kubernetes

1. [Follow the Kubernetes instructions to install kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

   :::note

   After installing kubeadm, do not power down or restart
   the host. Instead, continue directly to the next step.

   :::

1. As a regular user with sudo privileges, open a terminal on the host that you installed kubeadm on.

1. Initialize the control plane using the following command.

   ```bash
   sudo kubeadm init --pod-network-cidr=192.168.0.0/16 \
   --apiserver-cert-extra-sans=127.0.0.1
   ```

   :::note

   If 192.168.0.0/16 is already in use within your network you must select a different pod network
   CIDR, replacing 192.168.0.0/16 in the above command.

   :::

1. Execute the following commands to configure kubectl (also returned by `kubeadm init`).

   ```bash
   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   ```

1. Remove the taint from the control plane to allow Kubernetes to schedule pods on the control plane node.

   ```bash
   kubectl taint nodes --all node-role.kubernetes.io/master-
   ```

### Install {{prodname}}

1. [Configure a storage class for {{prodname}}.](../../../operations/logstorage/create-storage.mdx)

1. Install the Tigera operator and custom resource definitions.

   ```
   kubectl create -f {{filesUrl}}/manifests/tigera-operator.yaml
   ```

1. Install the Prometheus operator and related custom resource definitions. The Prometheus operator will be used to deploy Prometheus server and Alertmanager to monitor {{prodname}} metrics.

   :::note

   If you have an existing Prometheus operator in your cluster that you want to use, skip this step. To work with {{prodname}}, your Prometheus operator must be v0.40.0 or higher.

   :::

   ```
   kubectl create -f {{filesUrl}}/manifests/tigera-prometheus-operator.yaml
   ```

1. Install your pull secret.

   If pulling images directly from `quay.io/tigera`, you will likely want to use the credentials provided to you by your Tigera support representative. If using a private registry, use your private registry credentials instead.

   ```
   kubectl create secret generic tigera-pull-secret \
       --type=kubernetes.io/dockerconfigjson -n tigera-operator \
       --from-file=.dockerconfigjson=<path/to/pull/secret>
   ```

   For the Prometheus operator, create the pull secret in the `tigera-prometheus` namespace and then patch the deployment.

   ```
   kubectl create secret generic tigera-pull-secret \
       --type=kubernetes.io/dockerconfigjson -n tigera-prometheus \
       --from-file=.dockerconfigjson=<path/to/pull/secret>
   kubectl patch deployment -n tigera-prometheus calico-prometheus-operator \
       -p '{"spec":{"template":{"spec":{"imagePullSecrets":[{"name": "tigera-pull-secret"}]}}}}'
   ```

1. Install the Tigera custom resources. For more information on configuration options available in this manifest, see [the installation reference](../../../reference/installation/api.mdx).

   ```
   kubectl create -f {{filesUrl}}/manifests/custom-resources.yaml
   ```

   You can now monitor progress with the following command:

   ```
   watch kubectl get tigerastatus
   ```

   Wait until the `apiserver` shows a status of `Available`, then proceed to the next section.

### Install the {{prodname}} license

In order to use {{prodname}}, you must install the license provided to you by Tigera.

```
kubectl create -f </path/to/license.yaml>
```

You can now monitor progress with the following command:

```
watch kubectl get tigerastatus
```

When all components show a status of `Available`, proceed to the next section.

### Log in to {{prodname}} Manager

1. Create network admin user "Jane".

   ```
   kubectl create sa jane -n default
   kubectl create clusterrolebinding jane-access --clusterrole tigera-network-admin --serviceaccount default:jane
   ```

1. Extract the login `token` for use with the {{prodname}} UI.

   ```
   kubectl get secret $(kubectl get serviceaccount jane -o jsonpath='{range .secrets[*]}{.name}{"\n"}{end}' | grep token) -o go-template='{{.data.token | base64decode}}' && echo
   ```

   Copy the above `token` to your clipboard for use in the next step.

1. Set up a channel from your local computer to the {{prodname}} UI.

   ```
   kubectl port-forward -n tigera-manager svc/tigera-manager 9443
   ```

   Visit https://localhost:9443/ to log in to the {{prodname}} UI. Use the `token` from the previous step to authenticate.

### Secure {{prodname}} with network policy

To secure {{prodname}} component communications, install the following set of network policies.

```
kubectl create -f {{filesUrl}}/manifests/tigera-policies.yaml
```

Congratulations! You now have a single-host Kubernetes cluster with {{prodname}}.

## Next steps

- By default, your cluster networking uses IP in IP encapsulation with BGP routing. To review other networking options,
  see [Determine best networking option](../../../networking/determine-best-networking.mdx).
- [Get started with {{prodname}} tiered network policy](../../../network-policy/policy-tiers/tiered-policy.mdx)
