---
description: Upgrade to a newer version of Calico Enterprise installed with Helm.
---

import variables from '@site/calico-enterprise_versioned_docs/version-3.17/variables';

# Upgrade Calico Enterprise installed with Helm

import CodeBlock from '@theme/CodeBlock';

:::note

All upgrades in {variables.prodname} are free with a valid license.

:::

## Upgrades paths

Upgrading to {variables.prodname} v3.14 from previous {variables.prodname} versions other than v3.13 is currently unsupported.

:::note

Always check the [Release Notes](../../../../release-notes/index.mdx) for exceptions; limitations can override the above pattern.

:::

## Prerequisites

- Verify that your Kubernetes cluster is using Helm

  ```bash
  kubectl get tigerastatus
  ```

  If the result is successful, then your installation is using Helm.

## Prepare your cluster for the upgrade

During the upgrade the controller that manages Elasticsearch is updated. Because of this, the {variables.prodname} LogStorage
CR is temporarily removed during upgrade. Features that depend on LogStorage are temporarily unavailable, among which
are the dashboards in the Manager UI. Data ingestion is temporarily paused and will continue when the LogStorage is
up and running again.

To retain data from your current installation (optional), ensure that the currently mounted persistent volumes
have their reclaim policy set to [retain data](https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/).
Retaining data is only recommended for users that use a valid Elastic license. Trial licenses can get invalidated during
the upgrade.

If your cluster has Windows nodes and uses custom TLS certificates for log storage then, prior to upgrade, prepare and apply new certificates for [log storage](../../../../operations/comms/log-storage-tls.mdx) that include the required service DNS names.

### Default Deny

{variables.prodname} creates a default-deny for the calico-system namespace. If you deploy workloads into the calico-system namespace, you must create policy that allows the required traffic for your workloads prior to upgrade.

## Upgrade from 3.13 or later

:::note

These steps differ based on your cluster type. If you are unsure of your cluster type, look at the field `clusterManagementType` when you run `kubectl get installation -o yaml` before you proceed.

:::

1. Get the Helm chart

   <CodeBlock language='bash'>
     {'{variables.version}' === 'master'
       ? 'gsutil cp gs://tigera-helm-charts/tigera-operator-v0.0.tgz'
       : 'curl -O -L {variables.downloadsurl}/ee/charts/tigera-operator-{variables.chart_version_name}.tgz'}
   </CodeBlock>

1. Install the {variables.prodname} custom resource definitions.

   ```bash
   kubectl apply --server-side --force-conflicts -f {variables.filesUrl}/manifests/operator-crds.yaml
   kubectl apply --server-side --force-conflicts -f {variables.filesUrl}/manifests/prometheus-operator-crds.yaml
   kubectl apply --server-side --force-conflicts -f {variables.filesUrl}/manifests/eck-operator-crds.yaml
   ```

1. Run the Helm upgrade command for `tigera-operator` and make sure to either update `values.yaml` with your configuration or use custom `values.yaml` file:

  <CodeBlock language='bash'>
    {'{variables.version}' === 'master'
       ? (
         `helm upgrade calico-enterprise --values=<path-to-values-yaml> tigera-operator-v0.0.tgz \\
  --set-file imagePullSecrets.tigera-pull-secret=<path-to-pull-secret>,tigera-prometheus-operator.imagePullSecrets.tigera-pull-secret=<path-to-pull-secret> \\
  --namespace tigera-operator`
       )
       : (
         `helm upgrade calico-enterprise --values=<path-to-values-yaml> tigera-operator-{variables.chart_version_name}.tgz \\
  --set-file imagePullSecrets.tigera-pull-secret=<path-to-pull-secret>,tigera-prometheus-operator.imagePullSecrets.tigera-pull-secret=<path-to-pull-secret> \\
  --namespace tigera-operator`
       )
    }
  </CodeBlock>

1. You can monitor progress with the following command:

   ```bash
   watch kubectl get tigerastatus
   ```

:::note

Make sure you have the `CNI Type` defined in your `values.yaml` file, especially if it is different from the default `CNI` type.
If there are any problems you can use `kubectl get tigerastatus -o yaml` to get more details.

:::