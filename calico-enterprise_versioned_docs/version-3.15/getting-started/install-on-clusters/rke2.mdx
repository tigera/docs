---
description: Install Calico Enterprise on an RKE2 cluster.
---

import variables from '@site/calico-enterprise_versioned_docs/version-3.15/variables';
import CodeBlock from '@theme/CodeBlock';

# RKE2 - Rancher's Next Generation Kubernetes Distribution

## Big picture

Install {variables.prodname} on Rancher's Next-generation Kubernetes Distribution (RKE2) clusters.

## Before you begin

**CNI support**

Calico CNI for networking with {variables.prodname} network policy:

The geeky details of what you get:

<GeekDetails
  prodname='{variables.prodname}'
  details='Policy:Calico,IPAM:Calico,CNI:Calico,Overlay:VXLAN,Routing:BGP,Datastore:Kubernetes'
/>

**Required**

- A [compatible RKE2 cluster](../compatibility.mdx#rke2)

  For help, see [Rancher Kubernetes Engine cluster](https://rancher.com/docs/rke/latest/en).

- [Configure cluster with no CNI plugin](https://docs.rke2.io/reference/server_config#rke2-server-cli-help) using any of these methods:

  - RKE2 CLI: `--cni none`
  - Install script: `RKE2_CNI=none`
  - [Configuration file](https://docs.rke2.io/reference/server_config#rke2-server-cli-help): `cni: none`

- Cluster meets [system requirements](requirements.mdx)

- A [Tigera license key and credentials](calico-enterprise.mdx).

- A `kubectl` environment with access to your cluster

  - Ensure you have the [Kubeconfig file that was generated when you created the cluster](https://docs.rke2.io/cluster_access).

- If using a Kubeconfig file locally, [install and set up the Kubectl CLI tool](https://kubernetes.io/docs/tasks/tools/install-kubectl).

## How to

- [Install {variables.prodname}](#install-calico-enterprise)
- [Install the {variables.prodname} license](#install-the-calico-enterprise-license)

### Install {variables.prodname}

1. [Configure a storage class for {variables.prodname}.](../../operations/logstorage/create-storage.mdx).

1. In a v1.24 or earlier cluster, if the [PodSecurityPolicy admission controller](https://v1-24.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy) is enabled or it is a [CIS hardened cluster](https://docs.rke2.io/security/hardening_guide), install the required Tigera operator Pod Security Policy.

   <CodeBlock language='bash'>
   kubectl create -f {variables.filesUrl}/manifests/psp-tigera-operator.yaml
   </CodeBlock>

1. Install the Tigera operator and custom resource definitions.

   <CodeBlock language='bash'>
   kubectl create -f {variables.filesUrl}/manifests/tigera-operator.yaml
   </CodeBlock>

1. Install the Prometheus operator and related custom resource definitions. The Prometheus operator will be used to deploy Prometheus server and Alertmanager to monitor {variables.prodname} metrics.

   :::note

   If you have an existing Prometheus operator in your cluster that you want to use, skip this step. To work with {variables.prodname}, your Prometheus operator must be v0.40.0 or higher.

   :::

   <CodeBlock language='bash'>
   kubectl create -f {variables.filesUrl}/manifests/tigera-prometheus-operator.yaml
   </CodeBlock>

1. Install your pull secret.

   If pulling images directly from `quay.io/tigera`, you can use the credentials provided to you by your Tigera support representative. If using a private registry, use your private registry credentials instead.

   ```bash
   kubectl create secret generic tigera-pull-secret \
       --type=kubernetes.io/dockerconfigjson -n tigera-operator \
       --from-file=.dockerconfigjson=<path/to/pull/secret>
   ```

   For the Prometheus operator, create the pull secret in the `tigera-prometheus` namespace and then patch the deployment.

   ```bash
   kubectl create secret generic tigera-pull-secret \
       --type=kubernetes.io/dockerconfigjson -n tigera-prometheus \
       --from-file=.dockerconfigjson=<path/to/pull/secret>
   kubectl patch deployment -n tigera-prometheus calico-prometheus-operator \
       -p '{"spec":{"template":{"spec":{"imagePullSecrets":[{"name": "tigera-pull-secret"}]}}}}'
   ```

1. Install any extra [Calico resources](../../reference/resources/index.mdx) needed at cluster start using [calicoctl](../../reference/clis/calicoctl/overview.mdx).

1. Install the Tigera custom resources. For more information on configuration options available, see [the installation reference](../../reference/installation/api.mdx).

   <CodeBlock language='bash'>
   kubectl create -f {variables.filesUrl}/manifests/rancher/custom-resources-rke2.yaml
   </CodeBlock>

   Monitor progress with the following command:

   ```bash
   watch kubectl get tigerastatus
   ```

   Wait until the `apiserver` shows a status of `Available`, then proceed to the next section.

### Install the {variables.prodname} license

```bash
kubectl create -f </path/to/license.yaml>
```

Monitor progress with the following command:

```bash
watch kubectl get tigerastatus
```

## Next steps

**Recommended**

- [Configure access to {variables.prodname} Manager UI](../../operations/cnx/access-the-manager.mdx)
- [Authentication quickstart](../../operations/cnx/authentication-quickstart.mdx)
- [Configure your own identity provider](../../operations/cnx/configure-identity-provider.mdx)

**Recommended - Networking**

- The default networking uses VXLAN encapsulation with BGP routing. For all networking options, see [Determine best networking option](../../networking/determine-best-networking.mdx).

**Recommended - Security**

- [Get started with {variables.prodname} tiered network policy](../../network-policy/policy-tiers/tiered-policy.mdx)
