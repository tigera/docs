---
description: Install Calico on non-cluster hosts and VMs
---

# Install Calico on non-cluster hosts and VMs

## Big picture

Secure hosts and virtual machines (VMs) running outside of Kubernetes by installing $[prodname].

## Value

Calico Enterprise can also be used to protect hosts and VMs running outside of a Kubernetes cluster. In many cases, these are applications and workloads that cannot be easily containerized. $[prodname] lets you protect and gain visibility into these **non-cluster hosts** and use the same robust Calico network policy that you use for pods.

## Concepts

### Non-cluster hosts and host endpoints

A non-cluster host or VM is a computer that is running an application that is _not part of a Kubernetes cluster_. $[prodname] enables you to protect these hosts and VMs using the same Calico network policy that you use for workloads running inside a Kubernetes cluster. It also generates flow logs that provide visibility into the communication that host or VM is having with other endpoints in your environment.

In the following diagram, a Kubernetes cluster is running $[prodname] with networking (for pod-to-pod communication) and network policy; the non-cluster host uses Calico network policy for host protection and to generate flow logs for observability.

![non-cluster-host](/img/calico-enterprise/non-cluster-host.png)

For non-cluster hosts and VMs, you can secure host interfaces using **host endpoints**. Host endpoints can have labels that work the same as labels on pods/workload endpoints in Kubernetes. The advantage is that you can write network policy rules to apply to both workload endpoints and host endpoints using label selectors; where each selector can refer to the either type (or be a mix of the two). For example, you can easily write a global policy that applies to every host, VM, or pod that is running Calico.

To learn how to restrict traffic to/from hosts and VMs using Calico network policy, see [Protect hosts and VMs](../../network-policy/hosts/protect-hosts.mdx).

## Before you begin

**Required**

- Kubernetes API datastore is up and running and is accessible from the host

  If $[prodname] is installed on a cluster, you already have a datastore.

- Non-cluster host or VM meets $[prodname] [system requirements](../install-on-clusters/requirements.mdx)

  Ensure that your node OS includes the `ipset` and `conntrack` kernel dependencies.

## How to

### Set up your Kubernetes cluster to work with a non-cluster host or VM

1. Create a `NonClusterHost` custom resource.

    This resource enables the cluster-side ingestion endpoint to receive logs from non-cluster hosts. It also provides a dedicated Typha deployment to manage communication between the non-cluster host agent and Typha. To ensure proper operation, verify that the non-cluster hosts or VMs have network connectivity to your Kubernetes cluster.

    ```bash
    kubectl create -f - <<EOF
    apiVersion: operator.tigera.io/v1
    kind: NonClusterHost
    metadata:
      name: tigera-secure
    spec:
      endpoint: <https://domain-or-ip-address:port>
      typhaEndpoint: <domain-or-ip-address:port>
    EOF
    ```

    | Field         | Description                                                          | Accepted Values                                                          | Schema |
    | ------------- | -------------------------------------------------------------------- | ------------------------------------------------------------------------ | ------ |
    | endpoint      | Required. Location of the log ingestion point for non-cluster hosts. | Any HTTPS URL with a domain name and a port number                       | string |
    | typhaEndpoint | Required. Location of the Typha endpoint for non-cluster host agent and Typha communication. If you are using an ingress controller or an external load balancer, ensure it is configured to allow TCP Layer 4 passthrough. This is required for non-cluster host agent to establish a mutual TLS (mTLS) connection to the cluster. | Any IP address or domain name with a port number | string |

    Wait until the Tigera Manager and non-cluster Typha deployments reach the Available status before proceeding to the next step.

1. Create a `kubeconfig` file for your non-cluster host or VM:

    ```bash
    calicoctl nonclusterhost generate-config [--namespace=<namespace>] [--serviceaccount=<service-account>] [--certfile=<certificate-file>] > kubeconfig
    ```

    | Parameter      | Description                                                                                              | Default Values         |
    | -------------- | -------------------------------------------------------------------------------------------------------- | ---------------------- |
    | namespace      | Optional. The namespace where the service account for non-cluster hosts resides.                         | calico-system          |
    | serviceaccount | Optional. The service account used by non-cluster hosts to authenticate and securely access the cluster. | tigera-noncluster-host |
    | certfile       | Optional. Path to the file containing the PEM-encoded authority certificates. Use this option if you are providing your own [TLS certificates for Calico Enterprise Manager](../../operations/comms/manager-tls.mdx). If not specified, the Tigera root CA certificate will be used by default. | |

1. Create a [`HostEndpoint` resource](../../reference/host-endpoints/overview.mdx) for each non-cluster host or VM. The `node` and `expectedIPs` fields are required to match the hostname and the expected interface IP addresses.

### Set up your non-cluster host or VM

1. Configure the Calico Enterprise repository.

    ```bash
    curl -sfL https://downloads.tigera.io/ee/rpms/v3.22/calico_enterprise.repo -o /etc/yum.repos.d/calico-enterprise.repo
    ```

    Only Red Hat Enterprise Linux 8 and 9 x86-64 operating systems are supported in this version of $[prodname].

1. Install Calico node and fluent-bit log forwarder packages.

    - Use `dnf` to install the `calico-node` and `calico-fluent-bit` packages:

      ```bash
      dnf install calico-node calico-fluent-bit
      ```

1. Copy the `kubeconfig` created in cluster setup step 2 to host folder `/etc/calico/kubeconfig`.

1. Start Calico node and log forwarder.

    ```bash
    systemctl enable --now calico-node.service
    systemctl enable --now calico-fluent-bit.service
    ```

    You can configure the Calico node by tuning the environment variables defined in the `/etc/calico/calico-node/calico-node.env` file. For more information, see the [Felix configuration reference](../../reference/resources/felixconfig.mdx).

## Additional resources

- [Protect hosts and VMs](../../network-policy/hosts/protect-hosts.mdx)
