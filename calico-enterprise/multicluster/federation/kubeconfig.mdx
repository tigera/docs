---
description: Configure a local cluster to pull endpoint data from a remote cluster.
---

# Configure federated endpoint identity and multi-cluster networking

## Big picture

Configure a cluster to federate endpoint identities and establish cross-cluster connectivity.

## Value

Secure cross-cluster traffic with identity-aware policy, and leverage {{prodname}} to establish the required cross-cluster networking.

## Features

This how to guide uses the following {{prodname}} features:

- **RemoteClusterConfiguration** resource

## Concepts

### Local and remote clusters

Each cluster in the cluster mesh can act as both a local and remote cluster.

- Local clusters are configured to retrieve endpoint and routing data from remote clusters (via RemoteClusterConfiguration)
- Remote clusters authorize local clusters to retrieve endpoint and routing data

### Remote endpoint identity and policy

Typically, policy can only reference the endpoint identity (e.g. pod labels) of local endpoints. Federated endpoint identity enables local policy rules to reference remote endpoint identities.

### RemoteClusterConfiguration
RemoteClusterConfiguration is the resource that configures a local cluster to sync resources from a remote cluster. It primarily describes how a local cluster establishes that connection to the remote cluster through which resources are synced.

The resources synced through this connection enable the local cluster to reference remote endpoint identity and establish cross-cluster overlay routes.

RemoteClusterConfiguration creates this connection in one direction. If you want identity-aware policy on both sides (i.e. both clusters) of a connection, or you want {{prodname}} to establish cross-cluster overlay networking, you need to create a RemoteClusterConfiguration for both directions.

### kubeconfig files
Each cluster in the cluster mesh should have a dedicated kubeconfig file used by other clusters in the mesh to connect and authenticate.


## Before you begin
**Required**
- [Install {{prodname}}](../../getting-started/install-on-clusters/kubernetes/index.mdx)
- [Ensure pod IP routability](#ensure-pod-ip-routability)

## How to
- [Create kubeconfig files](#create-kubeconfig-files)
- [Create RemoteClusterConfiguration](#create-remoteclusterconfigurations)
- [Validate federation and multi-cluster networking](#validate-federation-and-multi-cluster-networking)
- [Create remote-identity-aware network policy](#create-remote-identity-aware-network-policy)
- [Troubleshoot](#troubleshoot)
- [Configure IP pool resources](#configure-ip-pool-resources)


### Ensure pod IP routability
Federation of workload endpoint identities requires [Pod IP routability](./overview#pod-ip-routability) between clusters. If your clusters are using a supported overlay networking mode, {{prodname}} can automatically meet this requirement when clusters are connected.

#### {{prodname}} multi-cluster networking
{{prodname}} can automatically extend the overlay networking in your clusters to establish pod IP routes across clusters and thus meet the requirement for Pod IP routability. Only VXLAN overlay is supported at this time.

Ensure the following requirements are met:
- All nodes in the cluster mesh can establish connections to each other via their private IP.
- VXLAN must be enabled on participating IP pools in all clusters and their IP pool CIDRs must not overlap.
- `routeSource` and `vxlan*` FelixConfiguration options must be aligned across clusters, and traffic on the `vxlanPort` must be allowed between nodes in the cluster mesh.
- RemoteClusterConfigurations must be established in both directions for cluster pairs in the cluster mesh.
- CNI must be Calico.

With these requirements met, multi-cluster networking will be automatically established when RemoteClusterConfigurations are created.

#### Other networking configurations
Alternatively, you can meet the requirement for Pod IP routability by configuring {{prodname}} with BGP or with VPC routing to establish unencapsulated Pod IP routes in your environment.


### Create kubeconfig files

Create a kubeconfig file, for each cluster, that will be used by other clusters to connect and authenticate themselves.

**For each** cluster in the cluster mesh, utilizing an existing kubeconfig with administrative privileges, follow these steps:

1. Create the ServiceAccount used by remote clusters for authentication:

   ```bash
   kubectl apply -f {{filesUrl}}/manifests/federation-remote-sa.yaml
   ```

1. If RBAC is enabled, create the ClusterRole and ClusterRoleBinding used by remote clusters for authorization:

   ```bash
   kubectl apply -f {{filesUrl}}/manifests/federation-rem-rbac-kdd.yaml
   ```

1. Create the kubeconfig file:

    Open a file in your favorite editor. Consider establishing a naming scheme unique to each cluster, e.g. `kubeconfig-app-a`.

    Paste the following into the file - we will replace the templated values with data retrieved in following steps.
   ```yaml
   apiVersion: v1
   kind: Config
   users:
     - name: tigera-federation-remote-cluster
       user:
         token: <YOUR-SERVICE-ACCOUNT-TOKEN>
   clusters:
     - name: tigera-federation-remote-cluster
       cluster:
         certificate-authority-data: <YOUR-CERTIFICATE-AUTHORITY-DATA>
         server: <YOUR-SERVER-ADDRESS>
   contexts:
     - name: tigera-federation-remote-cluster-ctx
       context:
         cluster: tigera-federation-remote-cluster
         user: tigera-federation-remote-cluster
   current-context: tigera-federation-remote-cluster-ctx
   ```

1. Retrieve the ServiceAccount token:

   #### If using Kubernetes >= 1.24
   - Create the ServiceAccount token:
   ```bash
   kubectl apply -f - <<EOF
   apiVersion: v1
   kind: Secret
   type: kubernetes.io/service-account-token
   metadata:
     name: tigera-federation-remote-cluster
     namespace: kube-system
     annotations:
       kubernetes.io/service-account.name: "tigera-federation-remote-cluster"
   EOF
   ```
   - Retrieve the ServiceAccount token value and replace `<YOUR-SERVICE-ACCOUNT-TOKEN>` with it's value:
   ```bash
   kubectl describe secret tigera-federation-remote-cluster -n kube-system
   ```

   #### If using Kubernetes < 1.24
   - Retrieve the ServiceAccount token value and replace `<YOUR-SERVICE-ACCOUNT-TOKEN>` with it's value:
   ```bash
   kubectl describe secret -n kube-system $(kubectl get serviceaccounts tigera-federation-remote-cluster -n kube-system -o jsonpath='{.secrets[0].name}')
   ```

1. Retrieve and save the certificate authority and server data:

   Run the following command:
   ```bash
   kubectl config view --flatten --minify
   ```
   Replace `<YOUR-CERTIFICATE-AUTHORITY-DATA>` and `<YOUR-SERVER-ADDRESS>` with `certificate-authority-data` and `server` values respectively.

1. Verify that the `kubeconfig` file works:

   Issue a command like the following to validate the kubeconfig file can be used to connect to the current cluster and access resources:
   ```bash
   kubectl --kubeconfig=kubeconfig-app-a get nodes
   ```

### Create RemoteClusterConfigurations
We'll now create the RemoteClusterConfigurations that establish synchronization between clusters. This enables remote-identity aware policy, federated services, and can establish multi-cluster networking.

**For each pair** of clusters in the cluster mesh (e.g. cluster A and cluster B):

1. In cluster A, create a secret that contains the kubeconfig for cluster B:

   Determine the namespace (`<namespace>`) for the secret to replace in all steps.
   The simplest method to create a secret for a remote cluster is to use the `kubectl` command because it correctly encodes the data and formats the file.
   ```bash
   kubectl create secret generic remote-cluster-secret-name -n <namespace> \
      --from-literal=datastoreType=kubernetes \
      --from-file=kubeconfig=<kubeconfig file>
   ```

1. If RBAC is enabled in cluster A, create a Role and RoleBinding that {{prodname}} will use to access the secret that contains the kubeconfig for cluster B:
   ```bash
   kubectl create -f - <<EOF
   apiVersion: rbac.authorization.k8s.io/v1
   kind: Role
   metadata:
     name: remote-cluster-secret-access
     namespace: <namespace>
   rules:
   - apiGroups: [""]
     resources: ["secrets"]
     verbs: ["watch", "list", "get"]
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: RoleBinding
   metadata:
     name: remote-cluster-secret-access
     namespace: <namespace>
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: Role
     name: remote-cluster-secret-access
   subjects:
   - kind: ServiceAccount
     name: calico-typha
     namespace: calico-system
   EOF
   ```

1. Create the RemoteClusterConfiguration in cluster A:

   Within the RemoteClusterConfiguration, we specify the secret used to access cluster B, and the overlay routing mode which toggles the establishment of cross-cluster overlay routes.
   ```yaml
   kubectl create -f - <<EOF
   apiVersion: projectcalico.org/v3
   kind: RemoteClusterConfiguration
   metadata:
     name: cluster-b
   spec:
     clusterAccessSecret:
       name: remote-cluster-secret-name
       namespace: <namespace>
       kind: Secret
     syncOptions:
       overlayRoutingMode: Enabled
   EOF
   ```

1. If you have IPPools in cluster A with NAT-outgoing enabled, you must [configure IP pool resources](#configure-ip-pool-resources).

1. Validate the that the remote cluster connection can be established using [calicoq](#validate-federation-and-multi-cluster-networking).

1. Repeat the above steps, switching cluster A and cluster B.


After completing the above steps for all cluster pairs in the cluster mesh, your clusters should now be ready to utilize remote-identity-aware policy and federated services, along with multi-cluster networking if requirements were met.

:::note
 This tutorial sets up RemoteClusterConfigurations in both directions. This is required for {{prodname}} to manage multi-cluster networking, and also ensures you can write identity-aware policy on both sides of a cross-cluster connection. Unidirectional connections can be made at your own discretion.
:::

### Validate federation and multi-cluster networking
#### RemoteClusterConfiguration & federated endpoint identity
[calicoq](../../operations/clis/calicoq/installing) can be used to ensure that a local cluster has connected to it's configured remote clusters. Use the following command:
```bash
calicoq eval "all()"
```
If all remote clusters are accessible, calicoq returns something like the following. Note the remote cluster prefixes: there should be endpoints prefixed with the name of each RemoteClusterConfiguration in the local cluster.
```
Endpoints matching selector all():
  Workload endpoint remote-cluster-1/host-1/k8s/kube-system.kube-dns-5fbcb4d67b-h6686/eth0
  Workload endpoint remote-cluster-1/host-2/k8s/kube-system.cnx-manager-66c4dbc5b7-6d9xv/eth0
  Workload endpoint host-a/k8s/kube-system.kube-dns-5fbcb4d67b-7wbhv/eth0
  Workload endpoint host-b/k8s/kube-system.cnx-manager-66c4dbc5b7-6ghsm/eth0
```

You can also view the [Prometheus metrics for Typha](../../reference/component-resources/typha/prometheus#metric-reference) to check the remote cluster connection status.

If either output contains unexpected results, proceed to the troubleshooting section.

#### Multi-cluster networking
If all requirements were met for {{prodname}} to establish multi-cluster networking, you can test the functionality by establishing a connection from a pod in a local cluster to the IP of a pod in a remote cluster. Ensure that there is no policy in either cluster that may block this connection.

If the connection fails, proceed to the [troubleshooting](#troubleshoot) section.

### Create remote-identity-aware network policy
With federated endpoint identity and routing between clusters established, you can now use labels to reference endpoints on a remote cluster in local policy rules, rather than referencing them by IP address.

The main policy selector still refers only to local endpoints; and that selector chooses which local endpoints to apply the policy.
However, rule selectors can now refer to both local and remote endpoints.

In the following example, cluster A (an application cluster) has a network policy that governs outbound connections to cluster B (a database cluster).
```yaml
apiversion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
    name: default.app-to-db
    namespace: myapp
spec:
    # The main policy selector selects endpoints from the local cluster only.
    selector: app == 'backend-app'
    tier: default
    egress:
    - destination:
        # Rule selectors can select endpoints from local AND remote clusters.
        selector: app == 'postgres'
      protocol: TCP
      ports: [5432]
      action: Allow
```

### Troubleshoot
#### RemoteClusterConfiguration & federated endpoint identity
For each impacted remote cluster pair (between cluster A and cluster B):
1. Retrieve the kubeconfig from the secret stored in cluster A. Manually verify that it can be used to connect to Cluster B.
   ```bash
   kubectl get secret -n <namespace> remote-cluster-secret-name -o=jsonpath="{.data.kubeconfig}" | base64 -d > verify_kubeconfig_b
   kubectl --kubeconfig=verify_kubeconfig_b get nodes
   ```
2. Validate that the Typha service account in Cluster A is authorized to retrieve the kubeconfig secret for Cluster B.
   ```bash
    kubectl auth can-i list secrets --namespace <namespace> --as=system:serviceaccount:calico-system:calico-typha
   ```
3. Repeat the above, switching cluster A to cluster B.

The output from [calicoq](#remoteclusterconfiguration--federated-endpoint-identity) may also provide insight into where issues may be occurring.

If the above steps provide no useful information, consider looking at the `typha` and `calico-node` logs for any relevant errors or warnings.


#### Multi-cluster networking
First, ensure that RemoteClusterConfiguration and federated endpoint identity are [functioning correctly](#remoteclusterconfiguration--federated-endpoint-identity-1).

Next, carefully validate that **all** [requirements](#calico-enterprise-multi-cluster-networking) for multi-cluster networking have been met. If all requirements are satisfied, consider:
* Validating that no network policy or firewall is causing packets to be dropped.
* Checking the `typha` and `calico-node` logs for any relevant errors or warnings.


### Configure IP pool resources
For local clusters with `NATOutgoing` configured on your IP pools, verify the following:

- Configure additional IP pools to cover the IP ranges of your remote clusters. This ensures that outgoing NAT is not performed on packets bound for the remote clusters.
- On the new IP pools, ensure that `disabled` is set to `true` to ensure the pools are not used for IP assignment on the local cluster.
- Verify that the IP pool CIDR used for pod IP allocation does not overlap with any of the IP ranges used by the pods and nodes of any other federated cluster.

For example, you can configure the following on your local cluster, referring to the `IPPool` on a remote cluster:

```yaml
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: cluster1-main-pool
spec:
  cidr: 192.168.0.0/18
  disabled: true
```

## Next steps

[Configure federated services](services-controller.mdx)
