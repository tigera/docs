---
description: Create a Calico Enterprise management cluster to manage multiple clusters from a single management plane.
---

import CodeBlock from '@theme/CodeBlock';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Create a management cluster

## Before you begin

**Required**

- A $[prodname] cluster, see [here](../../getting-started/install-on-clusters/index.mdx) for help
- A reachable, public IP address for the management cluster

For Helm installations, you also need:

- Helm 3 installed
- `kubeconfig` configured to work with your cluster (check by running `kubectl get nodes`)
- [Credentials for the Tigera private registry and a license key](../../getting-started/install-on-clusters/calico-enterprise.mdx)

## How to

<Tabs groupId="install-method">
<TabItem label="Operator" value="operator">

### Create a management cluster

To control managed clusters from your central management plane, you must ensure it is reachable for connections. The simplest way to get started (but not for production scenarios), is to configure a `NodePort` service to expose the management cluster. Note that the service must live within the `calico-system` namespace.

1.  Create a service to expose the management cluster.
    The following example of a NodePort service may not be suitable for production and high availability. For options, see [Port and service requirements](../reference/port-and-service-requirements.mdx).
    Apply the following service manifest.

    ```bash
    kubectl create -f - <<EOF
    apiVersion: v1
    kind: Service
    metadata:
      name: calico-manager-mcm
      namespace: calico-system
    spec:
      ports:
      - nodePort: 30449
        port: 9449
        protocol: TCP
        targetPort: 9449
      selector:
        k8s-app: calico-manager
      type: NodePort
    EOF
    ```

1.  Export the service port number, and the public IP or host of the management cluster. (Ex. "example.com:1234" or "10.0.0.10:1234".)
    ```bash
    export MANAGEMENT_CLUSTER_ADDR=<your-management-cluster-addr>
    ```
1.  Apply the [ManagementCluster](../../reference/installation/api.mdx#managementcluster) CR.

    ```bash
    kubectl apply -f - <<EOF
    apiVersion: operator.tigera.io/v1
    kind: ManagementCluster
    metadata:
      name: tigera-secure
    spec:
      address: $MANAGEMENT_CLUSTER_ADDR
    EOF
    ```

### Create an admin user and verify management cluster connection

To access resources in a managed cluster from the $[prodname] web console within the management cluster, the logged-in user must have appropriate permissions defined in that managed cluster (clusterrole bindings).

1. Create an admin user called, `mcm-user` in the default namespace with full permissions, by applying the following commands.

   ```bash
   kubectl create sa mcm-user
   kubectl create clusterrolebinding mcm-user-admin --serviceaccount=default:mcm-user --clusterrole=tigera-network-admin
   ```

1. Create a secret for the service account

   :::note

   This step is needed if your Kubernetes cluster is version v1.24 or above. Prior to Kubernetes v1.24, this secret is created automatically.

   :::

   ```bash
   kubectl apply -f - <<EOF
   apiVersion: v1
   kind: Secret
   type: kubernetes.io/service-account-token
   metadata:
     name: mcm-user
     annotations:
       kubernetes.io/service-account.name: "mcm-user"
   EOF
   ```

1. For Kubernetes v1.24+, use the following command to obtain the token for the secret associated with your host

   ```bash
   kubectl describe secret mcm-user
   ```

   For Kubernetes clusters prior to version v1.24, use the following command to retrieve your token:

   ```bash
   kubectl get secret $(kubectl get serviceaccount mcm-user -o jsonpath='{range .secrets[*]}{.name}{"\n"}{end}' | grep token) -o go-template='{{.data.token | base64decode}}' && echo
   ```

   In the top right banner, your management cluster is displayed as the first entry in the cluster selection drop-down menu with the fixed name, `management cluster`.

   ![Cluster Created](/img/calico-enterprise/mcm/mcm-management-cluster.png)

You have successfully installed a management cluster.

</TabItem>
<TabItem label="Helm" value="helm">

### Get the Helm chart

<CodeBlock language='bash'>
    {'$[version]' === 'master'
        ? `helm repo add tigera gs://tigera-helm-charts
helm repo update
helm pull tigera/tigera-operator --version $[releaseTitle]`
        : `helm repo add tigera-ee https://downloads.tigera.io/ee/charts
helm repo update
helm pull tigera-ee/tigera-operator --version $[releaseTitle]`}
</CodeBlock>

### Prepare the Installation Configuration

You **must** provide the desired configuration for your cluster via the `values.yaml`, otherwise installation will use the default settings based on the auto-detected provider.
The configurations you need to provide depends on your cluster's settings and your desired state.

Some important configurations you might need to provide to the installer (via `values.yaml`) includes (but not limited to): _kubernetesProvider_, _cni type_, or if you need to customize _TLS certificates_.

Here are some examples for updating `values.yaml` with your configurations:

Example 1. Providing `kubernetesProvider`:  if you are installing on a cluster installed by EKS, set the `kubernetesProvider` as described in the [Installation reference](../../reference/installation/api.mdx#provider)

  ```bash
  echo '{ installation: {kubernetesProvider: EKS }}' > values.yaml
  ```

Example 2. Providing custom settings in `values.yaml` for Azure AKS cluster with no Kubernetes CNI pre-installed:

  ```bash
  cat > values.yaml <<EOF
  installation:
    kubernetesProvider: AKS
    cni:
      type: Calico
    calicoNetwork:
      bgp: Disabled
      ipPools:
      - cidr: 10.244.0.0/16
        encapsulation: VXLAN
  EOF
  ```

For more information about configurable options via `values.yaml` please see [Helm installation reference](../../reference/installation/helm_customization).

### Install $[prodname]

<Tabs groupId="service-type">
<TabItem label="NodePort" value="NodePort">

To install a $[prodname] management cluster with Helm, using a NodePort service:

1. [Configure a storage class for Calico Enterprise](../../operations/logstorage/create-storage).

1. Export the service node port number

  ```bash
  export EXT_SERVICE_NODE_PORT=30449
  ```

  Export the public address or host of the management cluster. (Ex. "example.com:1234" or "10.0.0.10:1234".)

  ```bash
  export MANAGEMENT_CLUSTER_ADDR=<your-management-cluster-addr>:$EXT_SERVICE_NODE_PORT
  ```

1. Export one or more managed clusters.

  Generate the base64 encoded CRT and KEY for a managed cluster:

  ```bash
  openssl genrsa 2048 | base64 -w 0 > my-managed-cluster.key.base64
  openssl req -new -key <(base64 -d my-managed-cluster.key.base64) -subj "/CN=my-managed-cluster" | \
  openssl x509 -req -signkey <(base64 -d my-managed-cluster.key.base64) -days 365 | base64 -w 0 > my-managed-cluster.crt.base64
  ```

  Export the managed cluster variables:

  ```bash
  export MANAGED_CLUSTER_NAME=my-managed-cluster
  export MANAGED_CLUSTER_OPERATOR_NAMESPACE=tigera-operator
  export MANAGED_CLUSTER_CERTIFICATE=$(cat my-managed-cluster.crt.base64)
  ```

1. Append the management cluster context to your `values.yaml`:

  ```bash
  echo "
  managementCluster:
    enabled: true
    address: $MANAGEMENT_CLUSTER_ADDR
    service:
      enabled: true
      annotations:
      type: NodePort
      port: 9449
      targetPort: 9449
      protocol: TCP
      nodePort: $EXT_SERVICE_NODE_PORT

  managedClusters:
    enabled: true
    clusters:
    - name: $MANAGED_CLUSTER_NAME
      operatorNamespace: $MANAGED_CLUSTER_OPERATOR_NAMESPACE
      certificate: $MANAGED_CLUSTER_CERTIFICATE" >> values.yaml
  ```

1. Install the Tigera Operator and custom resource definitions using the Helm 3 chart:

  <CodeBlock language='bash'>
     {'$[version]' === 'master'
       ? `helm install $[prodnamedash] tigera/tigera-operator --version tigera-operator-v0.0 -f values.yaml \\
--set-file imagePullSecrets.tigera-pull-secret=<path/to/pull/secret>,tigera-prometheus-operator.imagePullSecrets.tigera-pull-secret=<path/to/pull/secret> \\
--set-file licenseKeyContent=<path/to/license/file/yaml> \\
--namespace tigera-operator --create-namespace`
       : `helm install $[prodnamedash] tigera-operator-$[chart_version_name].tgz -f values.yaml \\
--set-file imagePullSecrets.tigera-pull-secret=<path/to/pull/secret>,tigera-prometheus-operator.imagePullSecrets.tigera-pull-secret=<path/to/pull/secret> \\
--set-file licenseKeyContent=<path/to/license/file/yaml> \\
--namespace tigera-operator --create-namespace`}
  </CodeBlock>

1. You can now monitor progress with the following command:

  ```bash
  watch kubectl get tigerastatus
  ```

</TabItem>
<TabItem label="LoadBalancer" value="LoadBalancer">

To install a $[prodname] management cluster with Helm, using a LoadBalancer service:

#### Meet cloud provider requirements

Ensure that you have met the requirements for your cloud provider to provision a load balancer in your environment.

For example, if you are using EKS, you must meet the requirements defined in [create a network load balancer for AWS](https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html)

#### Install the management cluster

1. [Configure a storage class for Calico Enterprise](../../operations/logstorage/create-storage).

1. Export one or more managed clusters.

  Generate the base64 encoded CRT and KEY for a managed cluster:

  ```bash
  openssl genrsa 2048 | base64 -w 0 > my-managed-cluster.key.base64
  openssl req -new -key <(base64 -d my-managed-cluster.key.base64) -subj "/CN=my-managed-cluster" | \
  openssl x509 -req -signkey <(base64 -d my-managed-cluster.key.base64) -days 365 | base64 -w 0 > my-managed-cluster.crt.base64
  ```

  Export the managed cluster variables:

  ```bash
  export MANAGED_CLUSTER_NAME=my-managed-cluster
  export MANAGED_CLUSTER_OPERATOR_NAMESPACE=tigera-operator
  export MANAGED_CLUSTER_CERTIFICATE=$(cat my-managed-cluster.crt.base64)
  ```

1. Append the management cluster context to your `values.yaml`:

  ```bash
  echo "
  managementCluster:
    enabled: true
    service:
      enabled: true
      annotations:
      type: LoadBalancer
      port: 9449
      targetPort: 9449
      protocol: TCP

  managedClusters:
    enabled: true
    clusters:
    - name: $MANAGED_CLUSTER_NAME
      operatorNamespace: $MANAGED_CLUSTER_OPERATOR_NAMESPACE
      certificate: $MANAGED_CLUSTER_CERTIFICATE" >> values.yaml
  ```

  If you are using EKS, make sure your management cluster has the following annotations:
  ```yaml
  managementCluster:
    service:
      annotations:
      - key: service.beta.kubernetes.io/aws-load-balancer-type
        value: "external"
      - key: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type
        value: "instance"
      - key: service.beta.kubernetes.io/aws-load-balancer-scheme
        value: "internet-facing"
  ```

1. Install the Tigera Operator and custom resource definitions using the Helm 3 chart:

  <CodeBlock language='bash'>
     {'$[version]' === 'master'
       ? `helm install $[prodnamedash] tigera/tigera-operator --version tigera-operator-v0.0 -f values.yaml \\
--set-file imagePullSecrets.tigera-pull-secret=<path/to/pull/secret>,tigera-prometheus-operator.imagePullSecrets.tigera-pull-secret=<path/to/pull/secret> \\
--set-file licenseKeyContent=<path/to/license/file/yaml> \\
--namespace tigera-operator --create-namespace`
       : `helm install $[prodnamedash] tigera-operator-$[chart_version_name].tgz -f values.yaml \\
--set-file imagePullSecrets.tigera-pull-secret=<path/to/pull/secret>,tigera-prometheus-operator.imagePullSecrets.tigera-pull-secret=<path/to/pull/secret> \\
--set-file licenseKeyContent=<path/to/license/file/yaml> \\
--namespace tigera-operator --create-namespace`}
  </CodeBlock>

1. You can now monitor progress with the following command:

  ```bash
  watch kubectl get tigerastatus
  ```

#### Update the ManagementCluster address

1. Export the service port number

  ```bash
  export EXT_LB_PORT=<your-external-load-balancer-port>
  ```

  Export the public address or host of the management cluster, in this case the load-balancer's external IP (Ex. "example.com:1234" or "10.0.0.10:1234".)

  ```bash
  export MANAGEMENT_CLUSTER_ADDR=<your-load-balancer-external-addr>:$EXT_LB_PORT
  ```

  Replace the `address` field in the ManagementCluster resource.

  ```bash
  kubectl patch managementcluster tigera-secure --type merge -p "{\"spec\":{\"address\":\"${MANAGEMENT_CLUSTER_ADDR}\"}}"
  ```

</TabItem>
</Tabs>

### Create an admin user and verify management cluster connection

To access resources in a managed cluster from the $[prodname] web console within the management cluster, the logged-in user must have appropriate permissions defined in that managed cluster (clusterrole bindings).

Create an admin user, `mcm-user`, in the default namespace with full permissions, and token.

  ```bash
  kubectl create sa mcm-user
  kubectl create clusterrolebinding mcm-user-admin --clusterrole=tigera-network-admin --serviceaccount=default:mcm-user
  kubectl create token mcm-user -n default
  ```

  Use the generated token, to connect to the UI. In the top right banner in the UI, your management cluster is displayed as the first entry in the cluster selection drop-down menu with the fixed name, `management cluster`.

  Congratulations! You have now installed $[prodname] for a management cluster.

</TabItem>
</Tabs>

## Next steps

- [Create a managed cluster](create-a-managed-cluster.mdx)
- [Port and service requirements](../reference/port-and-service-requirements.mdx)
- [Change cluster type](change-cluster-type.mdx)
