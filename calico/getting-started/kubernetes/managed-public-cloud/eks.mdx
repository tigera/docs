---
description: Enable Calico network policy in EKS.
---

# Amazon Elastic Kubernetes Service (EKS)

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## Big picture

Enable $[prodname] in EKS managed Kubernetes service.

## Value

EKS has built-in support for $[prodname], providing a robust implementation of the full Kubernetes Network Policy API. EKS users wanting to go beyond Kubernetes network policy capabilities can make full use of the Calico Network Policy API.

You can also use $[prodname] for networking on EKS in place of the default AWS VPC networking without the need to use IP addresses from the underlying VPC. This allows you to take advantage of the full set of $[prodname] networking features, including $[prodname]'s flexible IP address management capabilities.

## How to

### Install EKS with Amazon VPC networking

The geeky details of what you get:

<GeekDetails
  prodname='$[prodname]'
  details='Policy:Calico,IPAM:AWS,CNI:AWS,Overlay:No,Routing:VPC Native,Datastore:Kubernetes'
/>

:::note

When using the Amazon VPC CNI plugin, $[prodname] does not support enforcement of network policy on IPv6 pods with `ENABLE_V4_EGRESS` set to `true`.

:::

***Prerequisites***

* You [disabled network policy for the AWS VPC CNI](https://docs.aws.amazon.com/eks/latest/userguide/network-policy-disable.html). 

1. First, create an Amazon EKS cluster.

   ```bash
   eksctl create cluster --name  <my-calico-cluster>
   ```

1. Install the Tigera operator and custom resource definitions.

   ```bash
   kubectl create -f $[manifestsUrl]/manifests/operator-crds.yaml
   kubectl create -f $[manifestsUrl]/manifests/tigera-operator.yaml
   ```

1. Configure the $[prodname] installation.

   ```bash
   kubectl create -f - <<EOF
   kind: Installation
   apiVersion: operator.tigera.io/v1
   metadata:
     name: default
   spec:
     kubernetesProvider: EKS
     cni:
       type: AmazonVPC
     calicoNetwork:
       bgp: Disabled
   EOF
   ```

1. Confirm installation by checking the `STATUS`, your cluster nodes should have a `Ready` status.

   ```
   kubectl get nodes -o wide
   ```

   It should return something like the following.

   ```
   NAME              STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME
   <your-hostname>   Ready    master   52m   v1.12.2   10.128.0.28   <none>        Ubuntu 18.04.1 LTS   4.15.0-1023-gcp   docker://18.6.1
   ```

### Install EKS with $[prodname] networking

The geeky details of what you get:

<GeekDetails
  prodname='$[prodname]'
  details='Policy:Calico,IPAM:Calico,CNI:Calico,Overlay:VXLAN,Routing:Calico,Datastore:Kubernetes'
/>

:::note

$[prodname] networking cannot currently be installed on the EKS control plane nodes. As a result the control plane nodes
will not be able to initiate network connections to $[prodname] pods. (This is a general limitation of EKS's custom networking support,
not specific to $[prodname].) As a workaround, trusted pods that require control plane nodes to connect to them, such as those implementing
admission controller webhooks, can include `hostNetwork:true` in their pod spec. See the Kubernetes API
[pod spec](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)
definition for more information on this setting.

:::

For these instructions, we will use `eksctl` to provision the cluster. However, you can use any of the methods in [Getting Started with Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html)

Before you get started, make sure you have downloaded and configured the [necessary prerequisites](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html#eksctl-prereqs)

1. First, create an Amazon EKS cluster without any nodes.

   ```bash
   eksctl create cluster --name my-calico-cluster --without-nodegroup
   ```

1. Since this cluster will use $[prodname] for networking, you must delete the `aws-node` daemon set to disable AWS VPC networking for pods.

   ```bash
   kubectl delete daemonset -n kube-system aws-node
   ```

1. Now that you have a cluster configured, you can install $[prodname].

<Tabs>
<TabItem label="Operator" value="Operator-0">

1. Install the Tigera operator and custom resource definitions.

   ```bash
   kubectl create -f $[manifestsUrl]/manifests/operator-crds.yaml
   kubectl create -f $[manifestsUrl]/manifests/tigera-operator.yaml
   ```

1. Configure the $[prodname] installation.

   ```bash
   kubectl create -f - <<EOF
   kind: Installation
   apiVersion: operator.tigera.io/v1
   metadata:
     name: default
   spec:
     kubernetesProvider: EKS
     cni:
       type: Calico
     calicoNetwork:
       bgp: Disabled
   EOF
   ```

1. Finally, add nodes to the cluster.

   ```batch
   eksctl create nodegroup --cluster <my-calico-cluster> --node-type t3.medium --max-pods-per-node 100
   ```

</TabItem>
<TabItem label="Manifest" value="Manifest-1">

1. Install the $[prodname] manifest.

   ```bash
   kubectl apply -f $[manifestsUrl]/manifests/calico-vxlan.yaml
   ```

1. Configure $[prodname] to disable AWS src/dst checks.

   ```bash
   kubectl -n kube-system set env daemonset/calico-node FELIX_AWSSRCDSTCHECK=Disable
   ```

1. Finally, add nodes to the cluster.

   ```bash
   eksctl create nodegroup --cluster my-calico-cluster --node-type t3.medium --max-pods-per-node 100
   ```

</TabItem>
<TabItem label="Helm" value="Helm-2">

1. Add $[prodname] into your Helm repository.

    ```batch
    helm repo add projectcalico https://docs.tigera.io/calico/charts
    ```

1. If $[prodname] is already added, update it to get the latest released version.

    ```batch
    helm repo update
    ```

1. Create the `tigera-operator` namespace.

   ```bash
   kubectl create namespace tigera-operator
   ```

1. Install version $[releaseTitle] of the $[prodname] operator and custom resource definitions.

    ```batch
    helm install calico projectcalico/tigera-operator --version $[releaseTitle] --namespace tigera-operator
    ```

1. Patch the CNI type with value `Calico`.

    ```batch
    kubectl patch installation default --type='json' -p='[{"op": "replace", "path": "/spec/cni", "value": {"type":"Calico"} }]'
    ```

1. Finally, add nodes to the cluster.

    ```batch
    eksctl create nodegroup --cluster my-calico-cluster --node-type t3.medium --max-pods-per-node 100
    ```

</TabItem>
</Tabs>

:::tip

   Without the `--max-pods-per-node` option above, EKS will limit the [number of pods based on node-type](https://github.com/awslabs/amazon-eks-ami/blob/main/nodeadm/internal/kubelet/eni-max-pods.txt). See `eksctl create nodegroup --help` for the full set of node group options.

:::

## Next steps

**Required**

- [Install calicoctl command line tool](../../../operations/calicoctl/install.mdx)

**Recommended**

- [Video: Everything you need to know about Kubernetes pod networking on AWS](https://www.projectcalico.org/everything-you-need-to-know-about-kubernetes-pod-networking-on-aws/)
- [Get started with Kubernetes network policy](../../../network-policy/get-started/kubernetes-policy/kubernetes-network-policy.mdx)
- [Get started with $[prodname] network policy](../../../network-policy/get-started/calico-policy/calico-network-policy.mdx)
- [Enable default deny for Kubernetes pods](../../../network-policy/get-started/kubernetes-default-deny.mdx)
